{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simplest Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DNN1(nn.Module):\n",
    "    def __init__(self, input_size: int, num_labels: int):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "                                        nn.Linear(input_size, 512), nn.ReLU(True),\n",
    "                                        nn.Linear(512, 512), nn.ReLU(True),\n",
    "                                        nn.Linear(512, num_labels),\n",
    "                                        )\n",
    "    def forward(self, input):\n",
    "        outputs = self.classifier(input)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev feature size: (379, 6487)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datapath = \"/data/lujd/algorithm2022/audioset/\"\n",
    "dev_df = pd.read_csv(datapath+\"dev.csv\", sep=\"\\t\")\n",
    "\n",
    "# dev set\n",
    "import numpy as np\n",
    "from Extract_feature import extract_mfcc\n",
    "\n",
    "n_mels = 26\n",
    "feature_option = \"mfcc\"\n",
    "dev_files = list(dev_df.filename)\n",
    "dev_labels = list(dev_df.scene_label)\n",
    "dev_features = []\n",
    "for filename in dev_files:\n",
    "    wav_file_path = datapath+filename\n",
    "    mfcc_feature = extract_mfcc(wav_file_path, n_mels=n_mels, option=feature_option)\n",
    "    dev_features.append(mfcc_feature.reshape(1,-1))             # [1, frames*n_features] (flatten)\n",
    "dev_features = np.concatenate(dev_features, axis=0)\n",
    "print(f\"dev feature size: {dev_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 128\n",
      "output size: torch.Size([128, 10])\n",
      "128 256\n",
      "output size: torch.Size([128, 10])\n",
      "256 379\n",
      "output size: torch.Size([123, 10])\n"
     ]
    }
   ],
   "source": [
    "unique_labels = ['airport', 'bus', 'metro', 'metro_station',\n",
    "                'park', 'public_square', 'shopping_mall',\n",
    "                'street_pedestrian', 'street_traffic', 'tram']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DNN1(dev_features.shape[1], len(unique_labels)).to(device)\n",
    "model.eval()\n",
    "\n",
    "batch_size = 128\n",
    "start_pos, end_pos = 0, 0\n",
    "with torch.no_grad():\n",
    "    while end_pos < len(dev_features):\n",
    "        end_pos = end_pos+batch_size if (end_pos+batch_size)<len(dev_features) else len(dev_features)\n",
    "        input = torch.Tensor(dev_features[start_pos:end_pos]).to(device)\n",
    "        print(start_pos, end_pos)\n",
    "        output = model(input)\n",
    "        start_pos = end_pos\n",
    "        print(f\"output size: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0091, -0.0346,  0.1381, -0.4813,  0.0720,  0.0573,  0.3161, -0.2161,\n",
       "          0.0751, -0.2733],\n",
       "        [ 0.0067, -0.0158,  0.1083, -0.2737,  0.0704,  0.0213,  0.1434, -0.1082,\n",
       "          0.0572, -0.1698],\n",
       "        [ 0.0231, -0.0075,  0.1276, -0.3178,  0.0343,  0.0272,  0.1563, -0.1500,\n",
       "          0.0561, -0.1808],\n",
       "        [ 0.0095,  0.0069,  0.0847, -0.2723,  0.0374,  0.0187,  0.1507, -0.1352,\n",
       "          0.0151, -0.1853],\n",
       "        [ 0.0011, -0.0277,  0.1109, -0.2673,  0.0473,  0.0580,  0.1613, -0.1307,\n",
       "          0.0427, -0.1590]], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6, 6, 6], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_labels = torch.argmax(output, dim=1)\n",
    "pre_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7730, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)           # [3, 5]\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)    # [3]\n",
    "output = loss(input, target)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process data for training DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Extract_feature import extract_mfcc\n",
    "\n",
    "datapath = \"/data/lujd/algorithm2022/audioset/\"\n",
    "\n",
    "class Audio_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_df, label_dict, feature_type):\n",
    "        super(Audio_Dataset, self).__init__()\n",
    "        self.file_list = audio_df.filename.to_list()\n",
    "        self.label_list = audio_df.scene_label.to_list()\n",
    "        self.label_dict = label_dict\n",
    "        \n",
    "        feature_list = []\n",
    "        for filename in self.file_list:\n",
    "            wav_file_path = datapath + filename\n",
    "            if feature_type==\"mfcc\" or feature_type==\"fbanks\":\n",
    "                feature = extract_mfcc(wav_file_path, option=feature_type)\n",
    "            feature_list.append(feature.reshape(1,-1))             # [1, frames*n_features] (flatten)\n",
    "        self.feature_array = np.concatenate(feature_list, axis=0)\n",
    "        print(self.feature_array.shape)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature, label = self.feature_array[index], self.label_list[index]\n",
    "        label = self.label_dict[label]\n",
    "        return feature, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 6487)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dev_df = pd.read_csv(datapath+\"dev.csv\", sep=\"\\t\")\n",
    "\n",
    "unique_labels = ['airport', 'bus', 'metro', 'metro_station', 'park', 'public_square',\n",
    "                 'shopping_mall', 'street_pedestrian', 'street_traffic', 'tram']\n",
    "label_dict = {}\n",
    "for ind, c in enumerate(unique_labels):\n",
    "    label_dict[c] = ind\n",
    "\n",
    "feature_option = \"mfcc\"\n",
    "dataset = Audio_Dataset(dev_df, label_dict, feature_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.8444,  1.3241,  0.7179,  ...,  0.2825,  0.1520,  0.3084],\n",
      "        [-8.3584,  2.8215,  0.3826,  ...,  0.2616,  0.0447,  0.1551],\n",
      "        [-4.2064,  1.8694,  0.6441,  ...,  0.2537, -0.1173, -0.0734],\n",
      "        [-3.8532,  1.1509, -0.4405,  ...,  0.0849,  0.0238, -0.0204],\n",
      "        [-2.3669,  2.2008, -0.3398,  ...,  0.1766, -0.0520, -0.1226]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for feature_list, label_list in dev_loader:\n",
    "    print(feature_list[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 6, 1, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "for feature_list, label_list in dev_loader:\n",
    "    print(label_list[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev feature size: torch.Size([64, 6487])\n",
      "output size: torch.Size([64, 10])\n",
      "dev feature size: torch.Size([64, 6487])\n",
      "output size: torch.Size([64, 10])\n",
      "dev feature size: torch.Size([64, 6487])\n",
      "output size: torch.Size([64, 10])\n",
      "dev feature size: torch.Size([64, 6487])\n",
      "output size: torch.Size([64, 10])\n",
      "dev feature size: torch.Size([64, 6487])\n",
      "output size: torch.Size([64, 10])\n",
      "dev feature size: torch.Size([59, 6487])\n",
      "output size: torch.Size([59, 10])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Extract_feature import extract_mfcc\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "n_mels = 26\n",
    "n_frames = 499\n",
    "n_features = 13\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DNN1(n_frames*n_features, len(unique_labels)).to(device)\n",
    "model.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
    "\n",
    "train_loss_list, true_label_list, pre_label_list = [], [], []\n",
    "for feature_list, label_list in dev_loader:\n",
    "    \n",
    "    print(f\"dev feature size: {feature_list.shape}\")\n",
    "\n",
    "    # forward\n",
    "    train_inputs = torch.Tensor(feature_list.float()).to(device)\n",
    "    train_labels = torch.LongTensor(label_list).to(device)\n",
    "    train_outputs = model(train_inputs)\n",
    "    train_loss = criterion(train_outputs, train_labels)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    pre_labels = torch.argmax(train_outputs, dim=1)\n",
    "    train_loss_list.append(train_loss.cpu().detach().numpy().item())\n",
    "    true_label_list.append(train_labels.cpu().detach().numpy())\n",
    "    pre_label_list.append(pre_labels.cpu().detach().numpy())\n",
    "    \n",
    "    print(f\"output size: {train_outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2866268157958984,\n",
       " 2.2864463329315186,\n",
       " 2.2715611457824707,\n",
       " 2.2785751819610596,\n",
       " 2.2386128902435303,\n",
       " 2.297173500061035]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label_list = np.concatenate(true_label_list)\n",
    "pre_label_list = np.concatenate(pre_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 8, 8, 1, 9, 5, 0, 6, 6, 3, 7, 9, 3, 3, 5, 4, 0, 5, 6, 2, 1,\n",
       "       2, 4, 3, 2, 8, 3, 1, 7, 6, 1, 7, 6, 9, 5, 2, 9, 6, 1, 3, 1, 4, 3,\n",
       "       0, 2, 5, 5, 8, 3, 9, 8, 7, 0, 5, 9, 7, 5, 0, 3, 9, 8, 6, 0, 7, 5,\n",
       "       3, 0, 6, 8, 7, 2, 9, 2, 7, 8, 2, 9, 8, 3, 6, 5, 8, 5, 8, 1, 0, 4,\n",
       "       3, 9, 5, 1, 4, 4, 7, 8, 7, 1, 1, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label_list[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_label_list[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11609498680738786"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(true_label_list == pre_label_list)/len(true_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.ones(5)\n",
    "np.expand_dims(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2],\n",
       "        [3, 4]],\n",
       "\n",
       "       [[5, 6],\n",
       "        [7, 8]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.array([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [5, 6, 7, 8]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reshape(2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.LongTensor([1,2,3,4,5])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=a.repeat((10,1))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.permute(1,0).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "       [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=a.repeat((10,1)).permute(1,0).reshape(-1).numpy()\n",
    "c.reshape(-1,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0469df6e709a2df7221cf6a41b4e84b201802bdff0592e7cbfef445384a2896"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
